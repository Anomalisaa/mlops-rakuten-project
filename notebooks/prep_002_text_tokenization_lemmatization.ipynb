{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e9f25-3563-48e9-a939-7861b6c66dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958db381-cd2e-469b-9595-77a7a9761492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode  \n",
       "0           10  \n",
       "1         2280  \n",
       "2           50  \n",
       "3         1280  \n",
       "4         2705  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('../data/raw/raw_x_y.csv')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbaa5fd-b375-40ca-8280-5c61a7f156fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing of the raw dataframe\n",
    "\n",
    "\n",
    "# Merge 'designation' and 'description', handling NaN in 'description'\n",
    "df_raw['text'] = df_raw['designation'] + ' ' + df_raw['description'].fillna('')\n",
    "\n",
    "# Step 1: Remove special characters but keep letters, numbers, and spaces\n",
    "df_raw['cleaned_text'] = df_raw['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', str(x) if pd.notnull(x) else ''))\n",
    "\n",
    "# Step 2: Convert cleaned text to lower case\n",
    "df_raw['cleaned_text'] = df_raw['cleaned_text'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Get stopwords for English and French\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "stop_words_fr = set(stopwords.words('french'))\n",
    "\n",
    "# Set custom stopwords \n",
    "custom_stopwords = set([\"chez\", \"der\", \"plu\", \"haut\", \"peut\", \"non\", \"100\", \"produit\", \"lot\", \"tout\", \"cet\", \"cest\", \"sou\", \"san\"])\n",
    "\n",
    "# Combine both sets of stopwords\n",
    "stop_words = stop_words_eng.union(stop_words_fr).union(custom_stopwords)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Function to remove stopwords from tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Tokenize, lowercase, lemmatize, and remove stopwords\n",
    "df_raw['lemmatized_text'] = df_raw['cleaned_text'].apply(\n",
    "    lambda x: remove_stopwords(lemmatize_tokens(word_tokenize(x.lower())))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43348a9a-9267-4851-af7d-aafcf2a723c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>olivia personalisiertes notizbuch  150 seiten ...</td>\n",
       "      <td>[olivia, personalisiertes, notizbuch, 150, sei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...</td>\n",
       "      <td>journal des arts le n 133 du 28092001  lart et...</td>\n",
       "      <td>[journal, art, 133, 28092001, lart, marche, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>[grand, stylet, ergonomique, bleu, gamepad, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>peluche donald  europe  disneyland 2000 marion...</td>\n",
       "      <td>[peluche, donald, europe, disneyland, 2000, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n",
       "      <td>la guerre des tuques luc a des ideacutees de g...</td>\n",
       "      <td>[guerre, tuques, luc, ideacutees, grandeur, ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "   prdtypecode                                               text  \\\n",
       "0           10  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1         2280  Journal Des Arts (Le) N째 133 Du 28/09/2001 - L...   \n",
       "2           50  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3         1280  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4         2705  La Guerre Des Tuques Luc a des id&eacute;es de...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  olivia personalisiertes notizbuch  150 seiten ...   \n",
       "1  journal des arts le n 133 du 28092001  lart et...   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3  peluche donald  europe  disneyland 2000 marion...   \n",
       "4  la guerre des tuques luc a des ideacutees de g...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [olivia, personalisiertes, notizbuch, 150, sei...  \n",
       "1  [journal, art, 133, 28092001, lart, marche, sa...  \n",
       "2  [grand, stylet, ergonomique, bleu, gamepad, ni...  \n",
       "3  [peluche, donald, europe, disneyland, 2000, ma...  \n",
       "4  [guerre, tuques, luc, ideacutees, grandeur, ve...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316c86af-bd34-4b54-8cad-c8efda7c9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_raw.drop(['designation', 'description', 'productid', 'text'], axis=1)\n",
    "\n",
    "# I keep both cleaned_text and lemmatized_text as I want to experiment a little further with the best option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35caddc5-32d5-4a84-a435-d6d4ace4e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I save the entire df_processed to pkl\n",
    "df_processed.to_pickle('../data/processed/df_text_processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (masterclass_venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
